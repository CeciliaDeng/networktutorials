---
title: "Accessing the _anuran_ API"
author: "Lisa Rottjers"
date: "24 januari 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## What does _anuran_ do?

The _anuran_ package contains functions that can be used to establish the existence of conserved or unique patterns across network. You can find _anuran_ [here](https://github.com/ramellose/anuran). At the moment, the [README](https://github.com/ramellose/anuran/blob/master/README.md) contains several standard commands and there are two vignettes available that demonstrate how to use _anuran_. The first vignette uses [the demo data](https://ramellose.github.io/anuran/demo_anuran.html) and looks at the soil microbiome in three different plots. The second vignette uses [a data set of soil samples from the Arctic](https://ramellose.github.io/networktutorials/workshop_demo.html). Both of these vignettes use the command line interface (CLI). This means that you run _anuran_ from your terminal and it will generate several files containing the outcomes of the analysis. 
However, you might want to tweak your analysis so you can compare your centrality value of choice, or you might want to use the null models to test the significance of network-level properties not currently supported by _anuran_. This is where the application programming interface (API) comes in. 

## What is an API exactly and why would you use it?

While a command line interface specifies how the command line input controls the software, the API specifies how you should interact with _anuran_ in Python. For example, you don't need to know how _anuran_ works to be able to generate null models with it. For nearly all functions in the package, the [Sphinx documentation](https://ramellose.github.io/anuran/index.html) describes its parameters and output. You can also get this documentation by importing the function and running _help(function name)_. 

In this demo, we will go through some of the most important functions that may be useful if you want to adapt _anuran_ specifically to your research. 
You will see how to export a json file of null models, and how to generate a custom centrality score. 

## How are the _anuran_ functions structured?

Each function in the package is part of a [module](https://ramellose.github.io/anuran/py-modindex.html). At the moment, there are 7 modules: centrality, draw, graphvals, nulls, sets, stats and utils. 
To use the API, the draw and utils modules can be ignored. The draw module contains code for generating figures with Seaborn, while the utils module contains functions that need to be imported by the other modules. For example, we generate the null models in parallel to reduce the computational time that _anuran_ needs. The actual edge swapping is therefore implemented in utils, so the null module can import it and run several instances in parallel. 
The centrality and graphvals modules contain functions for calculating properties from groups of networks. Unless you want to run these to extract intermediate files, you will probably also not use the modules directly. Instead, the most interesting modules are the nulls and sets modules. You can get the null models to do your own analysis, or you can define your own sets. For example, you might choose to define an intersection as a group of edges where two matching edges do not need to have the same association partners, but the association partners do need to be from the same family.  

In this case, we will generate some null models first. We will use two networks that we inferred from [the arctic soil data set](https://sfamjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1462-2920.2010.02277.x). with two different software methods. Download them from these links: [SPIEC-EASI](https://raw.githubusercontent.com/ramellose/networktutorials/master/Workshop%20network%20analysis%202020/arctic_spiec.graphml) and [FlashWeave](https://raw.githubusercontent.com/ramellose/networktutorials/master/Workshop%20network%20analysis%202020/arctic_flash.graphml) by right-clicking the page, selecting 'Save page as...' and saving them as .graphml files. 

First, we define all the parameters of the function. The location where you saved the two networks should be given as a list (the brackets []), since _anuran_ can accept multiple location sat once. _The number _n_ is the total number of null models to generate per network, while _core_ determines the number of processor cores to use. The fraction and prevalence are then used to define a synthetic core; in this case, we will generate five groups of null models. The first group will not have a core network, the second two will have a core network that is 5% of the total network size and occurs in half or all of the networks, while the last two will have a core network that is 10% of the total core size and also occurs in half or all of the networks. 

Keep in mind that especially the core prevalence needs to make sense: we cannot generate a core network with edges in 20% of networks, since we only have 2 networks. Additionally, the core network sets are derived from a single network. The two networks that have the same core will both be randomized from either the FlashWeave network or the SpiecEasi network, and therefore the set size depends strongly on the total size of these networks. 

```{r params, eval=FALSE}
location = ['C:/Documents/demo']
n = 10
core = 4
fraction = [0.05, 0.1]
prevalence = [0.5, 1]
```

After we have defined the parameters, we need to correctly import the networks from the folder. Since _anuran_ can import different groups, it retains this group structure in a Python dictionary where each key is the base name of the imported folder. We can just give the key an arbitrary name. We only need to define a single key-value pair, with the value being the list where the networks will be added. 

The next part is to get all the graph files from the folder. You can use glob, or you can just give the full filenames. 

Then, we iterate over the the files. For each file, we read the network into memory and then add it to the dictionary as a tuple. The tuple contains the name of the file (without the complete filepath) and a [NetworkX](https://networkx.github.io/) object. 

Because we add the network as a tuple, we can later on use the name if we want to find a specific null models. 

```{r network, eval=FALSE}
networks = {'demo': list()}

import glob
files = [f for f in glob.glob(location + "**/*.graphml", recursive=True)]
files = ['C:/Documents/demo/arctic_flash.graphml', 'arctic_spiec.graphml']
for file in files:
    network = nx.read_graphml(file)
    networks['demo'].append((os.path.basename(file), nx.to_undirected(network)))
```

If you call the networks dictionary, you will see the structure of this object. It should now look like this (with different memory codes):
```{r struc, eval=FALSE}
networks

{'demo': [('arctic_flash.graphml', <networkx.classes.graph.Graph at 0x1dc4c6b7fd0>),
          ('arctic_spiec.graphml', <networkx.classes.graph.Graph at 0x1dc4c796fd0>)]}

```

Next, we import the function that generates the null models (appropriately named generate_nulls) and run it. 
```{r import, eval=FALSE}
from anuran.nulls import generate_null
random, degree = generate_null(networks, n=n, core=core, fraction=fraction, prev=prevalence)
```

And there we go! Two new objects with all the null models we want. One problem: they are still in memory. We can write them to individual graph files, or generate a big json dump that will keep the structure. We will do both, so you learn how to access the random and degree dictionaries. 


